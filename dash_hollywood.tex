\documentclass[sigconf]{acmart}

\usepackage{booktabs} % For formal tables
\usepackage{hyperref}
\usepackage[l2tabu,orthodox]{nag}
\usepackage[utf8x]{inputenc}
\usepackage[british]{babel}
%\usepackage[babel=true]{microtype}
%\usepackage{amsmath}
%\usepackage[all]{onlyamsmath}
%\usepackage{newtxtext}
%\usepackage{newtxmath}
\usepackage{upquote}
\usepackage{graphicx}
\usepackage{url}
\usepackage{subfigure}
\usepackage{booktabs}
%\usepackage{bytefield}
%\usepackage{listings}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{color}
\usepackage{balance}
\usepackage{tabularx}
%\usepackage{rotating}
%\usepackage{mathtools}
\usepackage{makecell}
\usepackage{subfigure}
\usepackage{amsmath}
% Copyright
%\setcopyright{none}
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}
\newcommand{\todo}[1]{\textbf{\textcolor{red}{To do -- #1}}}
\graphicspath{{../figures/}}

\copyrightyear{2018} 
\acmYear{2018} 
\setcopyright{acmlicensed}
\acmConference[MMSys'18]{9th ACM Multimedia Systems Conference}{June 12--15, 2018}{Amsterdam, Netherlands}
\acmBooktitle{MMSys'18: 9th ACM Multimedia Systems Conference, June 12--15, 2018, Amsterdam, Netherlands}
\acmPrice{15.00}
\acmDOI{10.1145/3204949.3204959}
\acmISBN{978-1-4503-5192-8/18/06}

\begin{document}

\title{DASHing Towards Hollywood}

%\titlenote{Produces the permission block, and
%  copyright information}
%\subtitle{Extended Abstract}
%\subtitlenote{The full version of the author's guide is available as
%  \texttt{acmart.pdf} document}

\author{Saba Ahsan}
\affiliation{%
  \institution{Aalto University}
  \city{Espoo}
  \country{Finland}
}
\email{saba.ahsan@aalto.fi}

\author{Stephen McQuistin}
\affiliation{%
  \institution{University of Glasgow}
  \city{Glasgow}
  \country{UK}
}
\email{sm@smcquistin.uk}

\author{Colin Perkins}
\affiliation{%
  \institution{University of Glasgow}
  \city{Glasgow}
  \country{UK}
}
\email{csp@csperkins.org}

\author{J\"org Ott}
\affiliation{%
  \institution{TU Munich}
  \city{Munich}
  \country{Germany}
}
\email{ott@in.tum.de}

\renewcommand{\shortauthors}{S. Ahsan, S. McQuistin, C. Perkins, and J. Ott}


\begin{abstract}
Adaptive streaming over HTTP has become the de-facto standard for video streaming over the
Internet, partly due to its ease of deployment in a heavily ossified Internet. Though
performant in most on-demand scenarios, it is bound by the semantics of TCP, with
reliability prioritised over timeliness, even for live video where the reverse may be
desired. In this paper, we present an implementation of MPEG-DASH over TCP Hollywood, a
widely deployable TCP variant for latency sensitive applications. Out-of-order delivery
in TCP Hollywood allows the client to measure, adapt and request the
next video chunk even when the current one is only partially downloaded. Furthermore, the
ability to skip frames, enabled by multi-streaming and out-of-order delivery, adds resilience against stalling 
for any delayed messages. We observed
that in high latency and high loss networks, TCP Hollywood significantly
lowers the possibility of stall events and also supports better quality downloads in
comparison to standard TCP, with minimal changes to current adaptation algorithms. 
\end{abstract}


% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10002951.10003227.10003251.10003255</concept_id>
<concept_desc>Information systems~Multimedia streaming</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10003033.10003039.10003048</concept_id>
<concept_desc>Networks~Transport protocols</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Multimedia streaming}
\ccsdesc[500]{Networks~Transport protocols}

\keywords{dynamic adaptive streaming over HTTP, DASH, multimedia streaming, head-of-line blocking, transport layer multistreaming}


\maketitle

\input{sections/introduction}
\input{sections/hollywood}
\input{sections/dash}
\input{sections/methodology}
\input{sections/testing}
\input{sections/related}
\input{sections/conclusion}

\appendix
\section{Reproducibility}

To aid with reproducibility, we provide all of the source code used in generating the
results described in this paper, alongside a Makefile that describes and performs the
process of performing the experiments, processing and graphing the results, and produces
the paper. The source code for the paper is available at \url{http://dx.doi.org/10.5525/gla.researchdata.596}.

Our evaluations require a modified Linux kernel, and each run of the evaluation simulates
different network parameters. To manage this complexity, we have split the paper's build
process into a series of stages. In this section, we describe the inputs and outputs of
each stage; we also give approximate durations for each stage.
We start by describing the required dependencies.

\subsection*{Dependencies}

The experiments are conducted within virtual machines, that use Linux with the TCP
Hollywood kernel extensions and API installed. VirtualBox and Vagrant are used to manage
these virtual machines. Each experiment run involves streaming Big Buck Bunny over a 
simulated network; after this, FFmpeg is used to perform SSIM and PSNR analysis between
the streamed version and a reference copy. Finally, Python and R are used to analyse and
graph the results of the experiments. The versions that were used in our testing are shown
in brackets; other versions may work.

\begin{itemize}
\item FFmpeg (3.4.2)
\item Python (2.7)
\item R (3.4.3) and packages (rkvo, ggplot2, data.table)
\item Vagrant (2.0.2)
\item VirtualBox (5.2.6r120293)
\item xz (5.2.3)
\end{itemize}

The experiments use the TCP Hollywood kernel and API. These are located in the following
repositories, and the specified versions used by the experiments:

\begin{itemize}
\item Kernel: \url{https://github.com/lumisota/tcp-hollywood-linux} \\ (version \texttt{0bb4a643})
\item API layer: \url{https://github.com/lumisota/hollywood-api-video} (version \texttt{325c415f4492d0eeebc96af87bca94826aed1d75})
\end{itemize}

These versions of the TCP Hollywood kernel and API included with the source code for the
paper, in the data repository version described above.

For readability, whenever this section refers to TCP Hollywood, it is these versions that
have been used.

Mininet\footnote{\url{http://mininet.org}} (version 2.2.1) is used to simulate the network
for each run. This is installed, and run, within the TCP Hollywood Vagrant box, and is
not required to be installed on the host machine.

\subsection*{Stage 0 (15 minutes)}

Where the TCP Hollywood kernel source code has not been provided, the Makefile will pull
a copy of it, using the repository and version specified. This takes place within an
Ubuntu 14.04 virtual machine, which clones the repository, and compresses it into a 
tarball; this tarball is used in the next stage.

By performing this stage inside a Linux virtual machine, rather than on the host
machine, we avoid problems caused by case-insensitive file systems. The Linux kernel
assumes case-sensitivity, and there are a number of files in the same directory, whose
name only differs in case. These files would be deleted if the repository was cloned on
a host that had a case-insensitive file system; macOS, by default, uses such a file system.

The data repository package contains the TCP Hollywood source code, as a tarball, in the
stage0 directory.

\subsection*{Stage 1 (3-4 hours)}

TCP Hollywood is comprised of a set of modifications to the Linux kernel, and a user-space
API layer. Stage 1 involves building a virtual machine image that has the TCP
Hollywood kernel. Vagrant is used for this process; a clean Ubuntu 14.04 box is downloaded,
upon which the specified version of the TCP Hollywood kernel is downloaded and installed.

\subsubsection*{Inputs}
\begin{itemize}
\item TCP Hollywood kernel
\item Ubuntu 14.04 (trusty) Vagrant base box
\end{itemize}
\subsubsection*{Outputs}
\begin{itemize}
\item TCP Hollywood kernel Vagrant box
\end{itemize}

\subsection*{Stage 2 (15 minutes)}

The next stage is to install the TCP Hollywood API within the Vagrant box. This includes
installing the modified HTTP client and server used in the experiments. In addition,
mininet is installed, for use in simulating the network required by the experiments.

\subsubsection*{Inputs}
\begin{itemize}
\item TCP Hollywood kernel Vagrant box
\item TCP Hollywood API
\end{itemize}
\subsubsection*{Outputs}
\begin{itemize}
\item TCP Hollywood Vagrant box
\end{itemize}

\subsection*{Stage 3 (20-25 minutes per experiment)}

The third stage involves conducting the experiments themselves, with a virtual machine
(using the TCP Hollywood Vagrant box) instantiated for each run. The experiments involve
streaming Big Buck Bunny over network, simulated using Mininet. The network conditions are specified
by network profiles (listed in the Makefile as \texttt{SN\_RUNS} and \texttt{VN\_RUNS});
these are files that describe the network conditions (e.g., bandwidth, delay, loss rates).
For each profile, the experiment is run both with TCP and TCP Hollywood, to compare the
two. Finally, to produce the graphs in this paper, each run is repeated. We use 10
repetitions by default; this can be controlled using the \texttt{RUN\_NUMBERS} variable
in the Makefile. With two protocols (standard TCP and TCP Hollywood), 23 network profiles,
and 10 repetitions, the Makefile provided will perform, by default, 460 experiments.

Each run produces a set of files: logs from both the client and server, describing the
application-layer activity (e.g., what is sent or received); a tcpdump taken at the
server; and QoE logs (SSIM and PSNR analysis) that result from comparing the streamed
video to the reference copy.

\subsubsection*{Inputs}
\begin{itemize}
\item TCP Hollywood Vagrant box
\item Big Buck Bunny (as MPEG-DASH chunks)
\item Big Buck Bunny reference (1080p); \textasciitilde40GB file
\item Network profiles
\end{itemize}
\subsubsection*{Outputs}

Each experiment run produces:
\begin{itemize}
\item Client logfile
\item Server logfile
\item Server tcpdump
\item SSIM logfile
\item PSNR logfile
\end{itemize}

\subsection*{Stage 4 (5 minutes)}
The previous stage produces output files for each experiment, giving application-layer
activity, and QoE data. In this stage, these output files are aggregated processed,
allowing for analysis and graphing in the next stage.

Broadly, the experiments fall into two groups: those with static network conditions, that
do not change throughout the stream, and those with variable network conditions, where
parameters, such as bandwidth and delay, are changed every 30 seconds. The results of
experiments within each group, and for each network profile, are aggregated together.

\subsubsection*{Inputs}
\begin{itemize}
\item Stage 3 output files
\item Stage 4 Python and R scripts
\end{itemize}
\subsubsection*{Outputs}

Each group (static or variable) produces (aggregated by network profile):
\begin{itemize}
\item Aggregated PSNR data
\item Aggregated SSIM data
\item Aggregated QoS data (e.g., rebuffering events)
\item Aggregated bitrate data
\end{itemize}

\subsection*{Stage 5 (5 minutes)}
In this stage, the processed results files generated by the previous stage are graphed.
\begin{itemize}
\item Stage 4 output files
\item Stage 5 R scripts
\end{itemize}
\subsubsection*{Outputs}

\begin{itemize}
\item Figures 3 to 16 inclusive
\end{itemize}

\subsection*{Stage 6 (1 minute)}
With the results of the experiments graphed, the final stage is to build the paper.

\begin{itemize}
\item Paper TeX and BibTeX files
\item Figures (static and those generated in stage 5)
\end{itemize}
\subsubsection*{Outputs}
\begin{itemize}
\item Paper
\end{itemize}

\subsection*{Discussion}

The source code that we have made available, alongside this appendix, should be sufficient
for repeating the experiments conducted in this paper. However, making the paper
reproducibility introduces a number of challenges, and encounters various limitations. We
discuss and reflect on those in this section.

Each evaluation run makes use of a simulated network, run within a virtual machine. As
part of this simulation, random packet loss is introduced. As this is non-deterministic,
the results generated by building the paper will be different between different builds
including the published work. Typical approaches to managing randomness for repeatability
are made difficult by our use of virtual machines: where ordinarily a pseudo-random
number generator could be used, with a specified seed, our evaluations make use of the
\emph{system's} random number generator. By repeating our measurements a sufficient
number of times (the paper, and the code we provide, specifies 10 runs), we aim not only
to bolster the statistical significance of our results, but ensure that the trends we
discuss hold true when the measurements are repeated. However, where we discuss a
particular run (e.g., in Section \ref{sec:other_testing}), it is inevitable that the text
will not match results generated in other builds. It is not clear how non-determinism
should be handled when considering reproducibility, where pseudo-random number generators
cannot be used.

Our particular testing requirements (i.e., a modified Linux kernel) lend themselves to using
virtual machines, and orchestrating those machines programmatically (e.g., by using
Vagrant). This minimises the dependencies that need to be installed on the host machine:
for example, each run uses Mininet, but this runs within the virtual machine, rather than
on the host machine. However, this methodology also introduces limitations: our
evaluations essentially depend on a particular Linux environment, and packaging this for
reproducibility is non-trivial. While including the TCP Hollywood kernel and API code
goes some way towards preventing decay in the reproducibility of the paper, some
dependencies on external sources remain. For example, several software packages are
installed using package managers; this relies upon the availability of the underlying
repositories. There exists a trade-off between how long the paper can be usefully
reproduced using the assets we provide, and the tractability of identifying and including
\emph{all} of the dependencies that exist. Understanding this trade-off and making
appropriate choices is important if reproducibility is to be improved more generally.

\bibliographystyle{ACM-Reference-Format}
\bibliography{dash_hollywood}

\end{document}
